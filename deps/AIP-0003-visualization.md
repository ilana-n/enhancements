# Dynamo AIPerf Analyze and Visualize For Profiling Sweeps

**Status**: Draft

**Authors**: [ilana-n]

**Category**: Feature

**Replaces**: N/A

**Replaced By**: N/A

**Sponsor**: [TBD]

**Required Reviewers**: [TBD]

**Review Date**: [TBD]

**Pull Request**: [TBD]

**Implementation PR / Tracking Issue**: [TBD]

# Summary

Introduce `aiperf plot` command to generate interactive and static visualizations from profiling results. This addresses the common need to visualize performance metrics, compare multiple runs, and generate pareto curves for analysis and reporting.

# Motivation

Users frequently run profiling experiments across multiple configurations to understand performance trade-offs. Currently, there is no built-in way to visualize AIPerf results, forcing users to manually export data and use external tools (matplotlib, Excel, etc.) to create plots.

Common visualization needs include:
1. **Comparing multiple runs** - Understanding how different configurations (concurrency, model size, etc.) affect performance
2. **Analyzing single runs** - Identifying anomalies, warmup effects, and performance trends over time
3. **Interactive exploration** - Dynamically selecting metrics and plot types 

AIPerf can significantly improve the user experience by providing built-in visualization capabilities that work seamlessly with existing profiling workflows.

## Goals

* Support static artifact generation (PNG, HTML)
* Support comparing multiple profiling runs side-by-side
* Enable deep-dive analysis of single runs with time series and distribution plots
* Allow users to configure which plots are generated by default
* Provide interactive visualizations where users can dynamically select metrics and plot types in the browser

## Non Goals

* Real-time visualization during profiling (plots are generated post-profiling)
* Integration with external databases

### REQ 1 Input-Agnostic Visualization

AIPerf **MUST** provide a `plot` command that works with result files regardless of how they were generated. The `plot` command **MUST** support multiple input modes:
1. Single profile run results
2. Multiple profile run results (for manual comparison)
3. Directory containing multiple runs

### REQ 2 Static Visualization Export

AIPerf **MUST** support generating static visualization artifacts via `aiperf plot --format`:
- PNG format for individual plot images
- HTML format for self-contained interactive reports

Static exports **MUST** be stored in a `visualizations/` subdirectory and **SHOULD** be suitable for inclusion in reports and CI/CD pipelines. All formats **MUST** work with all input modes (single run, multiple runs, directory of runs).

### REQ 3 Interactive Visualization

AIPerf **MUST** provide an interactive web-based visualization interface via `aiperf plot --host`. The interface **MUST** support:
- **Dynamic axis selection**: Users choose any available metric for x-axis and y-axis
- **Run selection**: Toggle which runs to display
- **Plot type switching**: Switch between scatter, line, box plots, time series
- **Interactive controls**: Zoom, pan, hover for detailed values
- **Save from browser**: Export current plot as PNG or HTML directly from the dashboard
- **Filter controls**: Filter by request characteristics, time ranges

The interactive mode **MUST** run locally without requiring external services or internet connectivity. The server **MUST** provide download functionality for plots.

### REQ 4 Comprehensive Metric Support

The visualization system **MUST** support a comprehensive set of metrics for axis selection and plotting:
- Latency metrics (TTFT, ITL, E2E, percentiles)
- Throughput metrics (requests/sec, tokens/sec)
- Resource metrics (GPU memory, GPU utilization)
- Configuration parameters (concurrency, sequence lengths, batch size)
- Quality metrics (error rate, success rate)

The system **MUST** automatically detect which metrics are available in the provided result files.

### REQ 5 Essential Plot Types

The system **MUST** provide the following essential visualizations:
- **Comparison plots**: Pareto curves, metric vs parameter, distribution comparisons
- **Time series plots**: Per-request metrics over time (TTFT, ITL, E2E latency)
- **Distribution plots**: Histograms, box plots, violin plots for latency analysis

Users **SHOULD** be able to configure which plots are generated by default in static mode.

# Proposal

### Plot Command

**Design Principle**: Plot is **input-agnostic** - it works with any result files regardless of how they were generated.

Generates visualizations from stored results. Automatically detects whether to show sweep comparison or single-run analysis:
```bash
# Generate static PNG plots (default)
aiperf plot ./artifacts/dir

# Interactive hosted mode
aiperf plot ./artifacts/dir --host

# Generate HTML report
aiperf plot ./artifacts/dir --html

# Single run analysis
aiperf plot ./artifacts/dir/Qwen3-0.6B-concurrency8
aiperf plot ./artifacts/dir --run Qwen3-0.6B-concurrency8
```

**Mode Detection:**

The plot command automatically detects the visualization mode based on the provided path:
- **Sweep directory** (contains multiple run subdirectories) → Comparison mode
- **Single run directory** (contains metrics.json) → Deep-dive mode

**Default Behavior (Static PNG):**

By default, `aiperf plot` generates static PNG files:
- Saves plots to `{input_path}/visualizations/` directory

**Multi-Run Comparison Mode (Static):**

Preset default visualizations for sweep directories:
- Pareto curve (throughput vs latency)

Users can set their own default plots in the config. 

**Single-Run Analysis Mode (Static):**

Default visualizations for individual runs:
- TTFT (Time to First Token) per request over time
- ITL (Inter-token Latency) per request over time
- Request Latency over time
- Output Token Throughput Per User over time
- GPU memory over time

Users can set their own default plots in the config. 

**Interactive Hosted Mode (`--host`):**

Optional interactive web dashboard:
- Launches local web server
- Browser-based controls for:
  - Selecting which runs to include in comparison
  - Choosing x-axis/y-axis for plots
  - Switching plot types (line, scatter, bar)
  - Applying filters by parameter values
  - Zooming and panning through data
- Real-time plot updates without re-running command
- Useful for exploratory analysis 

**HTML Report Mode (`--html`):**

Optional self-contained HTML report:
- Generates single HTML file with embedded interactive plots
- Uses Plotly.js for interactivity (zoom, hover, pan)
- No web server required after generation
- Saves to `{input_path}/visualizations/report.html`

## Configuration

Users can configure default plots and make their own presets for multi-run comparison and single-run analysis in `~/.aiperf/config.yaml`:
```yaml
visualization:
  # =============================================================================
  # MULTI RUN COMPARISON: Default plots when comparing multiple runs
  # =============================================================================
  multi_run_defaults:
    - pareto_curve
    - ttft_vs_prefill_throughput
    - ttft_vs_decode_throughput
    - goodput_rate
  
  # =============================================================================
  # SINGLE RUN: Default plots for analyzing one run over time
  # =============================================================================
  single_run_defaults:
    - ttft_over_time
    - itl_over_time
    - latency_over_time
    - throughput_over_time
  
  # =============================================================================
  # MULTI RUN COMPARISON PRESETS
  # =============================================================================
  multi_run_plots:
    pareto_curve:
      name: "Pareto Curve"
      description: "Find optimal throughput/latency trade-offs on a concurrency sweep"
      x: request_latency
      y: request_throughput
      labels: concurrency
      highlight_pareto: true
    
    ttft_vs_prefill_throughput:
      name: "Prefill Performance" 
      description: "Compare time to first token against prefill throughput across different configurations"
      x: time_to_first_token
      y: prefill_throughput

    ttft_vs_decode_throughput:
      name: "Decode Performance"
      description: "Compare time to first token against output token throughput across different configurations"
      x: time_to_first_token
      y: output_token_throughput

    goodput_rate:
      name: "Success Rate"
      description: "Percentage of successful requests"
      metric: goodput_ratio
      type: bar
      format: percentage
      target_line: 99
      enabled_if: goodput_configured
  
  # =============================================================================
  # SINGLE RUN PRESETS (time-series over duration of profiling run)
  # =============================================================================
  single_run_plots:
    ttft_over_time:
      name: "TTFT Over Time"
      description: "Time to first token for each request"
      metric: ttft
      type: scatter
      x: request_time
      y: ttft
    
    itl_over_time:
      name: "Inter-Token Latency Over Time"
      metric: itl
      type: scatter
      x: request_time
      y: itl
    
    latency_over_time:
      name: "Request Latency Over Time"
      metric: request_latency
      type: scatter
      x: request_time
      y: request_latency
    
    gpu_memory_over_time:
      name: "GPU Memory Over Time"
      metric: gpu_memory_used
      type: line
      x: timestamp
      y: memory_gb
      enabled_if: dcgm_available
```
**[EXAMPLE SCREENSHOT: Interactive Dashboard - Multi Run Comparison Mode]**
![Example: Multi Run Comparison Dashboard](./deps/AIP-0003_images/dashboard-multirun.png)

**[EXAMPLE SCREENSHOT: Interactive Dashboard - Single Run Time Series]**
![Example: Single Run Time Series Dashboard](./deps/AIP-0003_images/dashboard-singlerun.png)

# Implementation

## Visualization Technology: Plotly Dash

The plot command is planned to use **Plotly Dash**, a Python framework for building interactive web applications.

**Key Benefits:**
- **Pure Python** - No JavaScript required, consistent with AIPerf's codebase
- **Interactive by design** - Users adjust settings (axes, filters, run selection) in browser without re-running commands
- **Self-contained** - Runs locally with no external services required
- **Customizable** - Full control over AIPerf-specific visualizations and styling

**Proposed Flow:**
```
aiperf plot ./my_sweep --host
    ↓
Load sweep data → Create Dash app → Launch server at localhost:8080
    ↓
Browser opens with:
  - Sidebar: controls for run selection, axes, plot types
  - Main area: interactive Plotly charts (zoom, pan, hover)
  - Tabs: switch between default visualizations
```

**Example Interactivity:**
```python
# When user changes dropdown, plot updates instantly via callback
@app.callback(Output('plot', 'figure'), Input('x-axis', 'value'))
def update_plot(x_axis):
    return generate_plot(x_axis=x_axis)
```

This approach should provide the customization needed for AIPerf's use cases. Alternative frameworks (Streamlit, React+Flask) could be considered during implementation if requirements change.

## Metrics for Visualization

The plot command must support all metrics available in AIPerf for x-axis and y-axis selection. The system automatically detects which metrics are available in the provided result files by parsing results.json and results.csv.

### Available Metrics by Category

| Category | Metric Name | Display Name / Header | Unit | Source | Percentiles Available |
|----------|------------|----------------------|------|--------|----------------------|
| **Latency Metrics** | `time_to_first_token` | Time to First Token (TTFT) | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `inter_token_latency` | Inter Token Latency (ITL) | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `request_latency` | Request Latency (E2E) | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `time_to_first_output_token` | Time to First Output Token (TTFO) | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `time_to_second_token` | Time to Second Token (TTST) | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `inter_chunk_latency` | Inter Chunk Latency (ICL) | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `credit_drop_latency` | Credit Drop Latency | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `stream_setup_latency` | Stream Setup Latency | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `stream_prefill_latency` | Stream Prefill Latency | milliseconds | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| **Throughput Metrics** | `request_throughput` | Request Throughput | requests/sec | results.json | avg only |
| | `output_token_throughput` | Output Token Throughput | tokens/sec | results.json | avg only |
| | `output_token_throughput_per_user` | Output Token Throughput Per User | tokens/sec/user | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `prefill_throughput` | Prefill Throughput | tokens/sec | results.json | avg only |
| | `goodput` | Goodput | tokens/sec | results.json | avg only |
| **Sequence Length Metrics** | `input_sequence_length` | Input Sequence Length (ISL) | tokens | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `output_sequence_length` | Output Sequence Length (OSL) | tokens | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `total_isl` | Total Input Sequence Length | tokens | results.json | sum |
| | `total_osl` | Total Output Sequence Length | tokens | results.json | sum |
| | `error_isl` | Error Input Sequence Length | tokens | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `total_error_isl` | Total Error Input Sequence Length | tokens | results.json | sum |
| **Token Count Metrics** | `output_token_count` | Output Token Count | tokens | results.json | sum |
| | `reasoning_token_count` | Reasoning Token Count | tokens | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `total_reasoning_tokens` | Total Reasoning Tokens | tokens | results.json | sum |
| **Request Count Metrics** | `request_count` | Request Count | count | results.json | total |
| | `good_request_count` | Good Request Count | count | results.json | total |
| | `error_request_count` | Error Request Count | count | results.json | total |
| **Efficiency Metrics** | `thinking_efficiency` | Thinking Efficiency | percent | results.json, results.csv | p1, p5, p10, p25, p50, p75, p90, p95, p99 |
| | `overall_thinking_efficiency` | Overall Thinking Efficiency | percent | results.json | avg only |
| **Time Metrics** | `benchmark_duration` | Benchmark Duration | seconds | results.json | total |
| | `min_request_timestamp` | Minimum Request Timestamp | nanoseconds | results.json | min |
| | `max_response_timestamp` | Maximum Response Timestamp | nanoseconds | results.json | max |
| **GPU Telemetry Metrics** | `gpu_power_usage` | GPU Power Usage | watts | GPU telemetry CSV | time series |
| | `power_management_limit` | GPU Power Limit | watts | GPU telemetry CSV | time series |
| | `energy_consumption` | Energy Consumption | megajoules | GPU telemetry CSV | time series |
| | `gpu_utilization` | GPU Utilization | percent | GPU telemetry CSV | time series |
| | `memory_copy_utilization` | Memory Copy Utilization | percent | GPU telemetry CSV | time series |
| | `gpu_memory_used` | GPU Memory Used | gigabytes | GPU telemetry CSV | time series |
| | `gpu_memory_free` | GPU Memory Free | gigabytes | GPU telemetry CSV | time series |
| | `gpu_memory_total` | GPU Memory Total | gigabytes | GPU telemetry CSV | time series |
| | `sm_clock_frequency` | SM Clock Frequency | megahertz | GPU telemetry CSV | time series |
| | `memory_clock_frequency` | Memory Clock Frequency | megahertz | GPU telemetry CSV | time series |
| | `memory_temperature` | Memory Temperature | celsius | GPU telemetry CSV | time series |
| | `gpu_temperature` | GPU Temperature | celsius | GPU telemetry CSV | time series |
| | `xid_errors` | XID Errors | count | GPU telemetry CSV | time series |
| | `power_violation` | Power Violation | microseconds | GPU telemetry CSV | time series |
| | `thermal_violation` | Thermal Violation | microseconds | GPU telemetry CSV | time series |
| **Configuration Parameters** | `concurrency` | Concurrency | count | config.json | fixed value |
| | `input_seq_len` | Input Sequence Length Config | tokens | config.json | fixed value |
| | `output_seq_len` | Output Sequence Length Config | tokens | config.json | fixed value |
| | `duration` | Benchmark Duration Config | seconds | config.json | fixed value |

# Alternate Solutions

## Alt 1: Single Command (Sweep + Auto-Visualize)

Automatically generate visualizations after sweep completes.

**Pros:**
- One command for everything
- Immediate results

**Cons:**
- Sweeps can take hours; users may want to visualize later
- Cannot re-visualize with different settings without re-running sweep
- Not flexible for CI/CD

**Reason Rejected:**
Separate commands provide better control and avoid re-running expensive profiling operations when users only want different visualizations or running visualizations when users only want raw profiling exports.

## Alt 2: External Tools (TensorBoard/WanDB)

Use existing visualization platforms.

**Pros:**
- No maintenance burden for UI/UX integrations. 
- Feature-rich

**Cons:**
- WanDB needs login and WiFi access
- Tensorboard is more specific to ML applications 
- Less control over AIPerf-specific visualizations

